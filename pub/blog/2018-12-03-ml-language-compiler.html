<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <title>Building a Language and Compiler for Machine Learning</title>
  <meta name="author" content="Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman, et al." />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Official website for the Julia programming language. Join the Julia community today.">

  <meta property="og:title" content="The Julia Language"/>
  <meta property="og:image" content="http://www.julialang.org/images/julia-open-graph.png"/>
  <meta property="og:description" content="Official website for the Julia programming language"/>

  
   <!-- Syntax highlighting via Prism, note: restricted langs -->
<link rel="stylesheet" href="/libs/highlight/github.min.css">
   
  

  <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i,700,700i" rel="stylesheet">
  <link rel="stylesheet" href="/assets/v2/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/assets/v2/css/app.css" />
  <link rel="stylesheet" href="/assets/v2/css/fonts.css" />
  <link rel="stylesheet" href="/assets/v2/css/highlight/github.css" />

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-28835595-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-28835595-1');
</script>

</head>

<body>

<div class="jd-content">
<p>Since we <a href="https://julialang.org/blog/2017/12/ml&amp;pl">originally proposed</a> the need for a first-class language, compiler and ecosystem for machine learning &#40;ML&#41;, there have been plenty of interesting developments in the field. Not only have the tradeoffs in existing systems, such as TensorFlow and PyTorch, not been resolved, but they are clearer than ever now that both frameworks contain distinct <a href="https://pytorch.org/docs/master/jit.html">“static graph”</a> and <a href="https://www.tensorflow.org/guide/eager">“eager execution”</a> interfaces. Meanwhile, the idea of ML models fundamentally being differentiable algorithms – often called <a href="https://www.facebook.com/yann.lecun/posts/10155003011462143">differentiable programming</a> – has caught on.</p>
<p>Where current frameworks fall short, several exciting new projects have sprung up that dispense with graphs entirely, to bring differentiable programming to the mainstream. <a href="https://github.com/mila-udem/myia">Myia</a>, by the Theano team, differentiates and compiles a subset of Python to high-performance GPU code. <a href="https://github.com/tensorflow/swift">Swift for TensorFlow</a> extends Swift so that compatible functions can be compiled to TensorFlow graphs. And finally, the <a href="https://github.com/FluxML/Flux.jl">Flux</a> ecosystem is extending Julia’s compiler with a number of ML-focused tools, including first-class gradients, just-in-time CUDA kernel compilation, automatic batching and support for new hardware such as TPUs.</p>
<p>All of these projects have enormous potential, but we think Julia has an edge. This post, based on our <a href="https://arxiv.org/abs/1811.01457">paper to be presented at NeurIPS MLSys</a>, will explore how we have used Julia to re-think ML tooling from the ground up, and provides some insight into the work that modern ML tools need to do.</p>
<div class="jd-toc"><ol><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#enter_flux">Enter Flux</a></li><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#taking_gradients">Taking Gradients</a></li><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#compiling_julia_for_gpus">Compiling Julia for GPUs</a></li><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#julia_on_tpus">Julia on TPUs</a></li><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#automatic_batching">Automatic Batching</a></li><li><a href="/pub/blog/2018-12-03-ml-language-compiler.html#conclusion">Conclusion</a></li></ol></div>
<h2 id="enter_flux"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#enter_flux">Enter Flux</a></h2>
<p>We need a language to write differentiable algorithms, and Flux takes Julia to be this language. Being designed from the ground up for mathematical and numerical computing, Julia is unusually well-suited for expressing ML algorithms.  Meanwhile, its mix of modern design and new ideas in the compiler makes it easier to address the high performance needs of cutting edge ML.</p>
<p>Where typical frameworks are all-encompassing monoliths in hundreds of thousands of lines of C&#43;&#43;, Flux is only a thousand lines of straightforward Julia code. Simply take <a href="https://github.com/FluxML/Zygote.jl">one package for gradients &#40;Zygote.jl&#41;</a>, <a href="https://github.com/JuliaGPU/CuArrays.jl/">one package for GPU support &#40;CuArrays.jl&#41;</a>, sprinkle with some light convenience functions, bake for fifteen minutes and out pops a fully-featured ML stack.</p>
<p>Like the other next-gen ML systems, Flux is committed to providing an intuitive &#40;“eager” or “define-by-run”&#41; interface, and takes a hard line against any kind of <a href="https://www.tensorflow.org/guide/autograph">graph building</a> or <a href="https://pytorch.org/docs/master/jit.html">performance annotations</a>. We support all of the language&#39;s features, from control flow and data structures to macros. Users can code interactively in Jupyter notebooks and combine high-performance numerics with convenient plotting and visualisation. But we also want to get the benefits traditionally held by “static graph” frameworks – zero-overhead source-to-source AD, operator fusion, multi-GPU/distributed training, and single-binary deployment.</p>
<p>How can we do all this? Effectively, we need to extract and analyse “static graphs” directly from written Julia syntax, which is in fact the entirely normal job of a <em>compiler</em>. Most ML systems problems turn out to be standard and well-studied compiler problems, viewed through the right lens. Using a compiled language is enough to solve many issues, and extending that compiler is the best way to solve many more. We cover just a sample of our current work in this field – namely taking gradients, compiling for GPUs and TPUs, and automatic batching.</p>
<h2 id="taking_gradients"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#taking_gradients">Taking Gradients</a></h2>
<p>Pushing the limits of reverse-mode differentiation, we have come to see this as a <a href="https://arxiv.org/abs/1810.07951">language-level problem</a>. Differentiation is a symbolic transformation, which is the domain of compilers. Existing frameworks achieve this by <em>tracing</em> &#40;effectively a form of <em>partial evaluation</em> or <em>abstract interpretation</em>&#41;. A new tensor type is introduced which records all the basic mathematical operations performed, yielding a graph &#40;or symbolic expression&#41; with the control flow and data structures of the host language removed. However, this presents a difficult tradeoff: we either accept the overhead of an interpreter &#40;eager execution&#41; or freeze user control flow and limit the kinds of models that can be built &#40;static graphs&#41;.</p>
<p>What if, instead, the “graph” were simply Julia’s own syntax? Taking this idea to its limit, we have built <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>, which works directly on SSA-form IR and supports language features like control flow, recursion, data structures and macros. We can then put the generated SSA-form adjoint code through a compiler such as <a href="http://llvm.org/">LLVM</a>, and get all the benefits of traditional compiler optimization applied to both our forward and backwards passes. In addition, this approach opens the opportunity to extend that compiler infrastructure with more advanced and domain-specific optimizations, such as kernel fusion and compilation to accelerators such as TPUs. Similar approaches are being explored by the <a href="https://gist.github.com/rxwei/30ba75ce092ab3b0dce4bde1fc2c9f1d">Swift for TensorFlow</a> and <a href="https://arxiv.org/abs/1810.11530">Myia</a> developers in a renaissance of source-to-source AD techniques.</p>
<p>A key advantage of Julia for this task is that it can be used to implement fundamental numerical libraries, like <a href="http://juliadiffeq.org/">differential equations solvers</a> or <a href="https://github.com/JuliaOpt/JuMP.jl">optimisation libraries</a>; this neatly solves a growing need in the ML community, in which researchers backpropagate through high-performance code such as <a href="https://people.csail.mit.edu/tzumao/diffrt/">ray tracers</a> and <a href="https://arxiv.org/abs/1611.01652">physics engines</a>, but the gradients must still be implemented by hand in C&#43;&#43;. In contrast, since Julia’s implementations are written in Julia, everything from <a href="https://github.com/FluxML/model-zoo/blob/a243e8b192236c30064fcdb7a36f17f3b6823c34/other/diffeq/diffeq.jl">ODEs</a> to <a href="https://wilmott.com/automatic-for-the-greeks/">financial pricing models</a> can be differentiated with ease. Bringing these powerful tools into models is where deep learning truly becomes differentiable programming.</p>
<h2 id="compiling_julia_for_gpus"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#compiling_julia_for_gpus">Compiling Julia for GPUs</a></h2>
<p>GPU programming is an essential part of modern ML. But the GPU is often treated as an implementation detail; frameworks provide kernels internally, but the user only sees a limited set of mathematical operations and can’t program the GPU directly. In contrast, GPU programming in Julia is <a href="https://devblogs.nvidia.com/gpu-computing-julia-programming-language/">first-class</a> all the way down to CUDA kernels &#40;which can happily be written and run from a script or notebook&#41;.</p>
<p>A simple vector addition kernel looks similar to the CUDA C equivalent. <pre><code class="language-julia">function kernel_vadd(a, b, c)
    i = (blockIdx().x-1) * blockDim().x + threadIdx().x
    c[i] = a[i] + b[i]
    return
end</code></pre> However, Julia&#39;s type specialization enables a powerful set of additional abstractions on the GPU. For example, the code above is not restricted to dense arrays of floats, and could instead be given sparse arrays of complex numbers; Julia&#39;s normal specialization mechanisms would generate a new set of PTX instructions on the fly. We can even abstract this code further into a “higher-order kernel” that accepts the <code>&#43;</code> function &#40;or <code>*</code>, or arbitrary user-defined <code>f</code>&#41; and thus create a whole family of functions <code>map&#40;f, x, y&#41;</code> in <a href="http://mikeinnes.github.io/2017/08/24/cudanative.html">four lines of code</a>.</p>
<p>This enables some powerful tricks, even if you never write CUDA code yourself. For example, we can transparently fuse a large broadcast expression like <code>1 / &#40;1 &#43; exp&#40;-x&#41;&#41;</code>, <em>and</em> its backwards pass, into a single GPU kernel, getting <a href="https://arxiv.org/abs/1810.08297">significant speedups</a>. We expect the native GPU code generation capabilities and ecosystem will power various Julia based machine learning libraries going forward.</p>
<h2 id="julia_on_tpus"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#julia_on_tpus">Julia on TPUs</a></h2>
<p>Taking this one step further, Google recently opened up the XLA IR used by their Cloud TPUs, making it possible for both other frameworks and users outside of ML to take advantage of this heavyweight hardware. XLA is powerful but limited: it can’t run a Python interpreter, certainly not with good performance. Frameworks then end up in a similar position as with gradients – they have no choice but to <a href="https://github.com/google/jax">use program tracing</a> to pry away the Python, and end up with a fast but much more limited ML language.</p>
<p><a href="https://arxiv.org/abs/1810.09868">Our response is predictable</a>: we only need to extract the “static graph” from written Julia programs and compile it directly to XLA, allowing Julia itself to run on TPUs. &#40;In fact, this is just a simple extension of Julia’s usual compilation process, which extracts the largest possible “static subgraphs” from your program before sending them to LLVM.&#41; This lets us take full advantage of the expressiveness of the Julia language, including control flow, recursion, multiple dispatch, higher-order functions, powerful data structures and abstractions, custom numeric types, and existing packages like differential equations solvers and linear algebra routines. All of this runs while reaping the benefits of the high-performance systolic array engine within the TPU. You can <a href="https://github.com/JuliaTPU/XLA.jl">try it today</a>, with examples for both <a href="https://github.com/JuliaTPU/XLA.jl/blob/d04c5914bc0d9f7d7fed68233f167d5b67003f7f/examples/resnet/resnet.jl">large ML models like ResNet</a> and <a href="https://github.com/JuliaTPU/XLA.jl/blob/d04c5914bc0d9f7d7fed68233f167d5b67003f7f/examples/tsvd.jl">linear algebra routines like TSVD</a>.</p>
<h2 id="automatic_batching"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#automatic_batching">Automatic Batching</a></h2>
<p>To get the most from these accelerators – which can have significant overheads per kernel launch, but scale very well over input size – it is common to <em>batch</em> programs, applying the forwards and backwards passes to multiple training examples at once. In simple cases, such as with convolutional nets, it’s simple to handle this by concatenating, say, 10 images along an extra batch dimension. But this task becomes much harder when dealing with variably-structured inputs, such as trees or graphs.</p>
<p>Most researchers address this by taking on the significant burden of batching code by hand. Different solutions have been proposed for different frameworks &#40;<a href="https://dynet.readthedocs.io/en/latest/tutorials_notebooks/Autobatching.html">DyNet</a>,  <a href="https://github.com/tensorflow/fold">TensorFlow Fold</a>, which heuristically try to batch some high level operations together when possible, but these typically either have their own usability issues or do not achieve the performance of hand-written code.</p>
<p>We suggest that this problem is identical to that of Single Program Multiple Data &#40;SPMD&#41; programming, which has been <a href="https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/blelloch/papers/Ble90.pdf">well-studied</a> by the language and compiler community for decades, and becomes visible in more recent approaches to batching like <a href="https://github.com/salesforce/matchbox">matchbox</a>. Indeed, it is very similar to the model of parallelism used by GPUs internally, and has been implemented as a compiler transform for the <a href="https://ispc.github.io/">SIMD units of CPUs</a>. Taking inspiration from this work, we are implementing the <a href="http://compilers.cs.uni-saarland.de/projects/wfv/">same transform</a> in Julia to provide SPMD programming both for scalar SIMD units and for model-level batching. This allows us to reach the ideal of writing simple code that operates on individual samples, while still getting the best performance on modern hardware.</p>
<h2 id="conclusion"><a href="/pub/blog/2018-12-03-ml-language-compiler.html#conclusion">Conclusion</a></h2>
<p>We believe that the future of machine learning rests in language and compiler technology, and in particular, in extending new or existing languages to meet the high demands of ML research. This is good not just for the ML community, but for numerical programming in general; languages that can support differentiation, vectorisation and exotic hardware well will be powerful enough to drive many advancements in science.</p>
<p>There is some way to go before these next-generation tools – Myia, Swift/TF and <a href="http://fluxml.ai">Flux</a> – are as production-ready as their existing framework counterparts, TensorFlow, PyTorch, and <a href="https://github.com/denizyuret/Knet.jl">Knet</a>. But if you’re breaking new ground in ML, they might well be your best bet. Give them a go, and see what the future of machine learning looks like.</p>

<head>
  <meta name="description" content="We thank our contributors, donators, and Fastly for their support in keeping the Julia Language going. Donate here to help pay for Julia's needs."/>
</head>

<footer class="container-fluid footer-copy">
    <div class="container">
      <div class="row">
        <div class="col-md-10 py-2">
          <p>
            We thank <a style="color: #7a95dd" href="https://www.fastly.com">Fastly</a> for their generous infrastructure support. Donations help pay for community resources such as CI, Discourse, workshops, travel, JuliaCon, and other such needs.
          </p>
          <p>
            ©2020-01-19 JuliaLang.org contributors. The website content uses the <a style="color: #7a95dd" href="https://github.com/JuliaLang/www.julialang.org/blob/master/LICENSE.md">MIT license</a>.
          </p>
        </div>
        <div class="col-md-2 py-2">
          <a class="btn btn-success" href="https://numfocus.org/donate-to-julia">Donate</a>
        </div>
      </div>
    </div>
</footer>

</div>
    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
    <script src="/assets/v2/js/jquery.min.js"></script>
<script src="/assets/v2/js/bootstrap.min.js"></script>
<script src="/assets/v2/js/platform.js"></script>
<script src="/assets/v2/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

  </body>
</html>
